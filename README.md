# Scale-invariant representation of machine learning
The frequency of the internal representation of deep neural network follows power laws regardless of the type of learning. The scale-invariant distribution implies that machine largely compresses frequent typical data, and at the same time, differentiates many atypical data as outliers. In this project, the power law phenomenon that appears in the hidden layer of the fully-connected deep neural network is demonstrated. The following models are used depending on the learning type. 
- Supervised learning: Label classification model 
- Self-supervised learning: Autoencoder
- Unsuervised learning: Restricted Boltzmann Machine  

![스크린샷 2022-01-19 오후 7 50 33](https://user-images.githubusercontent.com/42707786/150115923-96706358-fe12-45b6-a002-f025a1620975.png)

# Reference
- Sungyeop Lee and Junghyo Jo, "Scale-invariant representation of machine learning", arXiv preprint arXiv:22109.02914 (2021). 
