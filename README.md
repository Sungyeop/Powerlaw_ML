# Scale-invariant representation of machine learning
The frequency of the internal representation of deep neural network follows power laws regardless of the type of learning. The scale-invariant distribution implies that machine largely compresses frequent typical data, and at the same time, differentiates many atypical data as outliers. In this projects, the power law phenomenon that appears in the hidden layer of the fully-connected deep neural network is demonstrated. The following three models were used depending on the learning type. 
- Supervised learning      : Label classification model 
- Self-supervised learning : Autoencoder
- Unsuervised learning     : Restricted Boltzmann Machine  



# Reference
- Sungyeop Lee and Junghyo Jo, "Scale-invariant representation of machine learning", arXiv preprint arXiv:22109.02914 (2021). 
